{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "import collections\n",
    "import copy\n",
    "import os, errno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from __future__ import division\n",
    "from nltk.book import *\n",
    "import random\n",
    "from scipy.sparse import *\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import operator\n",
    "import pandas\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to find the most important words in the books according to the TFIDF criterion. This criterion consists of 2 parts:\n",
    "\n",
    "  * TF: Term frequency, in each book see how much time each word appears\n",
    "  * IDF: Inverse document frequency, see how important this word is compared to its apparition in other books.\n",
    "  \n",
    "The result is a TFIDF matrix, in our case the rows are corresponding to the books and the colmuns to the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We compute the TFIDF matrix on our books\n",
    "filenames=['./books/'+f for f in os.listdir('./books/')]\n",
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word', \n",
    "           stop_words='english',input='filename')\n",
    "book_tfidf = vect.fit_transform(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the TFIDF matrix we can visualise it in a word cloud. We have done a word cloud for the 7 different books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-20af83c1f370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbooknames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./books/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#we will remove the text that is in the title of the books from the TFIDF count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtextbooknames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'listdir' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "booknames=[f[:-4] for f in listdir('./books/')]\n",
    "\n",
    "#we will remove the text that is in the title of the books from the TFIDF count\n",
    "textbooknames=''\n",
    "for bookname in booknames:\n",
    "    textbooknames+=bookname.lower()+' '\n",
    "booktitles=nltk.regexp_tokenize(textbooknames,r'[a-zA-Z]+')\n",
    "\n",
    "#list of words corresponding to the colums of the matrix\n",
    "word_list=vect.get_feature_names()\n",
    "\n",
    "N_wordcloud=book_tfidf.multiply(5000)\n",
    "\n",
    "#picture locations for the wordcloud\n",
    "listpicture=[loc+f for f in os.listdir('./pictures_WC/')]\n",
    "\n",
    "#we first create a long string to feed to the wordcloud\n",
    "for counterbook, book in enumerate(booknames):\n",
    "    print book\n",
    "    s=''\n",
    "\n",
    "    for  counterword,word in enumerate(word_list):\n",
    "\n",
    "        if word not in booktitles:\n",
    "            n=int(round(N_wordcloud[counterbook,counterword]))\n",
    "            s+=(word+' ')*n\n",
    "\n",
    "\n",
    "    # plot and save the wordcloud\n",
    "    mask_HP = np.array(Image.open(listpicture[np.mod(counterbook,4)]))\n",
    "    wordcloud = WordCloud(background_color=\"white\",max_font_size=40,collocations = False,mask=mask_HP).generate(s)\n",
    "    fig=plt.figure()\n",
    "    fig.set_size_inches(15,10)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    fig.savefig('Wordcloud_'+book)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import text of the Books    \n",
    "f = open(\"books/Book 1 - The Philosopher_s Stone.txt\")\n",
    "text1 = f.read()\n",
    "\n",
    "f = open(\"books/Book 2 - The Chamber of Secrets.txt\")\n",
    "text2 = f.read()\n",
    "\n",
    "f = open(\"books/Book 3 - The Prisoner of Azkaban.txt\")\n",
    "text3 = f.read()\n",
    "\n",
    "f = open(\"books/Book 4 - The Goblet of Fire.txt\")\n",
    "text4 = f.read()\n",
    "\n",
    "f = open(\"books/Book 5 - The Order of the Phoenix.txt\")\n",
    "text5 = f.read()\n",
    "\n",
    "f = open(\"books/Book 6 - The Half Blood Prince.txt\")\n",
    "text6 = f.read()\n",
    "\n",
    "f = open(\"books/Book 7 - The Deathly Hallows.txt\")\n",
    "text7 = f.read()\n",
    "\n",
    "texts=[text1,text2,text3,text4,text5,text6,text7]\n",
    "\n",
    "for n,i in enumerate(texts):\n",
    "    texts[n]=i.replace('\"',\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pandas.read_excel('dataset spells.xlsx')\n",
    "values = df['Incantation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of things to exclude\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords+=[\".\",\",\",\"-\",\"?\",';',\":\",\"'\",\"'\",\"--\",\"``\",\"''\"]\n",
    "stopwords = set(stopwords)\n",
    "exclude_words=stopwords\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "texts=[text1,text2,text3,text4,text5,text6,text7]\n",
    "\n",
    "tokens=[[],[],[],[],[],[],[]]\n",
    "\n",
    "for n,text in enumerate(texts):\n",
    "\n",
    "    tokens[n]= nltk.regexp_tokenize(text,r'[a-zA-Z]+')\n",
    "    tokens[n]=[t.lower() for t in tokens[n] if t.lower() not in exclude_words if len(t)>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq=FreqDist(tokens[6])\n",
    "\n",
    "#fig= plt.figure()\n",
    "#fig.set_size_inches(15,4)\n",
    "#freq.plot(75,cumulative=True, figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_list=[item.lower() for val in values for item in val.split()]\n",
    "val_list=set(val_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirt=['skin','spell' ,'ears','curse','flame','cheering'\n",
    ",'sticking','point','flying','line','pack','head','switching'\n",
    ",'freezing','gripping','babbling','stinging','avis','unbreakable'\n",
    ",'extinguishing','conjunctivitis','aqua','bubble','prior','cave','drought','jinx']\n",
    "\n",
    "for d in dirt:\n",
    "    val_list.remove(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultName=[]\n",
    "resultValue=[]\n",
    "result=[]\n",
    "for val in val_list:\n",
    "    if val in freq:\n",
    "        \n",
    "        #print \"%s : %s\"%(val,freq[val])\n",
    "        result.extend([val,freq[val]])\n",
    "        resultName.append(val)\n",
    "        resultValue.append(freq[val])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(freq)\n",
    "\n",
    "#fig= plt.figure()\n",
    "#fig.set_size_inches(15,4)\n",
    "#result.plot(75,cumulative=True, figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
