{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome in the Harry Potter universe !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose this dataset because we are big fans of Harry Potter and we are interested in using our python skills on something we are passionate about. We know the communities and the characters. We also remember crying, being scared and laughing a lot in our childhood while we were watching the movies or reading the books. \n",
    "\n",
    "There is a big universe and a lot of data about Harry Potter. It was easy to find the books and the analysis seemed interesting. There are some words that are typical to Harry Potter such as the spells or locations. So, we could also analyse them. Creating a network in this universe made also a lot of sense to us.\n",
    "\n",
    "The challenge is to find relevant information using Python, we want to find some results that we expect and find some other results that will help us to think more deeply and get a better understanding about the Harry Potter universe. Our general knowledge on the topic encourages us to use this dataset and test our Python skill on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our Data Set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we started working with the Wikipedia data of Harry Potter. This dataset was quite small. We only had about 150 nodes in the network. After the presentation we had help from the other group that also worked with the Harry Potter data. So, in the end, we managed to work with the data from the wikia fandom of Harry Potter. This data is way more complete than the Wikipedia data. In our Giant connected component we now have 415 nodes. \n",
    "\n",
    "The dataset of wikia is in the form of an xml file it can be downloaded from [here](http://harrypotter.wikia.com/wiki/Special:Statistics). Once unzipped with an appropriate program this dataset has a size of 174 Mb.\n",
    "\n",
    "We also have the 7 Harry Potter books, a list of Harry Potter spells and the 8 Harry Potter movie subtitles.\n",
    "\n",
    "More information on the dataset: how we dowloaded and extracted the data and basics stats are in **Notebook 1**. More informaton on books and movies are in **Notebook 2 and 3**. All the books and subtitles can be dowloaded form here: **MISSING LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we just said we first started working with the wikipedia data. It was a real mess. We were very happy to finally manage to work with the wikia dataset of Harry Potter. The dataset was way bigger and contained too much information for us. We thus decided to take only pages that talked about characters. Concretely we used this [link](http://harrypotter.wikia.com/wiki/Special:Categories?&limit=5000) to find important character categories. We selected these categories: Gryffindors, Slytherins, Hufflepuffs, Ravenclaws, Death Eaters, First Order of the Phoenix, Dumbledore's Army, Muggles, Order of the Phoenix allies, Mascots and Hagrid's pets. We choose these categories because they capture the main characters in the Harry Potter universe.\n",
    "\n",
    "Once we had these we could extract the characters associated with these categories and construct the network.\n",
    "We will create a link from node a to node b if the name of node b appears on the wikia page of node a. Once we have the network, we notice that the graph is not connected. We thus only take the giant connected component. This is the graph from which we have been working. Please look at **Notebook 1** for details and implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the books and movieâ€™s subtitles, it was really easy to download from internet. The process of reading and extracting the relevant information of these txt and srt files is done in **Notebook 3**. Generally speaking, we just divided the books into words (tokenization) and splited the subtitles timeline into equal spots.\n",
    "\n",
    "After this we did a sentimentental analysis which is explained in **Notebook 3**. We also find the most frequent words in the books using TF-IDF with Word Cloud and analyze the spells which is explained in **Notebook2**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
