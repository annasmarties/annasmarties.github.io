{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome in the Harry Potter universe !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose this dataset because we are big fans of Harry Potter and we are interested in using our python skills on something we are passionate about. We know the communities and the characters. We also remember crying, being scared and laughing a lot in our childhood while we were watching the movies or reading the books. \n",
    "\n",
    "There is a big universe and a lot of data about Harry Potter. It was easy to find the books and the analysis seemed interesting. There are some words that are typical to Harry Potter such as the spells or locations. So, we could also analyse them. Creating a network in this universe made also a lot of sense to us.\n",
    "\n",
    "The challenge is to find relevant information using Python, we want to find some results that we expect and find some other results that will help us to think more deeply and get a better understanding about the Harry Potter universe. Our general knowledge on the topic encourages us to use this dataset and test our Python skill on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we started working with the Wikipedia data of Harry Potter. This dataset was quite small. We only had about 150 nodes in the network. After the presentation we had help from the other group that also worked with the Harry Potter data. So, in the end, we managed to work with the data from the wikia fandom of Harry Potter. This data is way more complete than the Wikipedia data. In our Giant connected component we now have 415 nodes. \n",
    "\n",
    "The dataset of wikia is in the form of an xml file it can be downloaded from [here](http://harrypotter.wikia.com/wiki/Special:Statistics). Once unzipped with an appropriate program this dataset has a size of 174 Mb.\n",
    "\n",
    "We also have the 7 Harry Potter books, a list of Harry Potter spells and the 8 Harry Potter movie subtitles.\n",
    "\n",
    "More information on the dataset: how we dowloaded and extracted the data and basics stats are in **[Notebook 1](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/Harry%20Potter%20Network%20notebook.ipynb)**. More informaton on books and movies are in **Notebooks [2](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/TFIDF-spell%20analysis.ipynb) and [3](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/ProjectB_Sentiment%20Analysis.ipynb)**. All the books and subtitles can be dowloaded form [here](https://github.com/annasmarties/annasmarties.github.io/tree/master/Dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we just said we first started working with the wikipedia data. It was a real mess. We were very happy to finally manage to work with the wikia dataset of Harry Potter. The dataset was way bigger and contained too much information for us. We thus decided to take only pages that talked about characters. Concretely we used this [link](http://harrypotter.wikia.com/wiki/Special:Categories?&limit=5000) to find important character categories. We selected these categories: Gryffindors, Slytherins, Hufflepuffs, Ravenclaws, Death Eaters, First Order of the Phoenix, Dumbledore's Army, Muggles, Order of the Phoenix allies, Mascots and Hagrid's pets. We choose these categories because they capture the main characters in the Harry Potter universe.\n",
    "\n",
    "Once we had these we could extract the characters associated with these categories and construct the network.\n",
    "We will create a link from node a to node b if the name of node b appears on the wikia page of node a. Once we have the network, we notice that the graph is not connected. We thus only take the giant connected component. This is the graph from which we have been working. Please look at **[Notebook 1](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/Harry%20Potter%20Network%20notebook.ipynb)** for details and implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concerning the books and movie’s subtitles, it was really easy to download from internet. The process of reading and extracting the relevant information of these txt and srt files is done in **[Notebook 3](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/ProjectB_Sentiment%20Analysis.ipynb)**. Generally speaking, we just divided the books into words (tokenization) and splited the subtitles timeline into equal spots.\n",
    "\n",
    "After this we did a sentimentental analysis which is explained in **[Notebook 3](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/ProjectB_Sentiment%20Analysis.ipynb)**. We also find the most frequent words in the books using TF-IDF with Word Cloud and analyze the spells which is explained in **[Notebook2](http://nbviewer.jupyter.org/github/annasmarties/annasmarties.github.io/blob/master/ipynb/TFIDF-spell%20analysis.ipynb)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "We were happy to find some meaningful results  with the different tools that we used. Some happy moments matched perfectly the books and the movies, some books had a lot of certain spells with a pretty obvious underlying reason. We also found some relevant communities with the Louvain algorithm. \n",
    "We did our best to show these results in comprehensive plots, that were logical for non scientific people. So all in all, we think we have some nice and clear outputs.\n",
    "\n",
    "### If we could borrow Hermione’s Time-Turner, ...\n",
    "\n",
    "…. we could analyse more deeply our different communities or analyse some other communities in the huge Harry Potter universe.\n",
    "\n",
    "…. we could analyse a lot of different sets of Harry Potter words, such as locations, objects, ...\n",
    "\n",
    "…. we could improve the sentimental analysis by also applying the tools of the TextBook package to the books and not just applying it to the subtitles as we have done now. It would be better to adapt these tools to the books in order to have the same scale. We had some issues on how to deal with the .srt file formatting. For this reason we decided to do two different analysis with two different tools (TextBook package and the sentiment function that we learned in the lecture). However, when we compared the results of the books and the movies with these different methods, we made the comparison of the books and subtitles without using the values. Thus, our results are just discussions on the extreme points of the graphs created with these different tools. This could be improved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
